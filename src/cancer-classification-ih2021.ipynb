{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install timm -q\n!pip install albumentations --upgrade -q\n!pip install segmentation_models_pytorch -q\n\n# General imports.\nimport gc\nimport os\nimport cv2\nimport timm\nimport torch\nimport random\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport albumentations as A\nimport segmentation_models_pytorch\n\n\n# Specific Imports.\nfrom torch import nn\nfrom tqdm import tqdm\nfrom tensorflow import keras\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom kaggle_datasets import KaggleDatasets\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom segmentation_models_pytorch.encoders import get_encoder\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom segmentation_models_pytorch.base import initialization as init\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb -qqq\nimport wandb\nfrom wandb.keras import WandbCallback\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"def seed_everything(SEED):\n    os.environ['PYTHONHASHSEED']=str(SEED)\n    random.seed(SEED)\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    \ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Brief Descriptive Analysis and EDA (EDA can be done later)","metadata":{}},{"cell_type":"code","source":"# Ref: https://www.kaggle.com/andrewmvd/isic-2019.\n# Note: Everything is done within a Kaggle Notebook.\n\ntrain_val_test_img_path = r\"/kaggle/input/isic-2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input\"\ngt_path = r\"/kaggle/input/isic-2019/ISIC_2019_Training_GroundTruth.csv\"\nmetadata_path = r\"/kaggle/input/isic-2019/ISIC_2019_Training_Metadata.csv\"\n\nground_truth_df = pd.read_csv(gt_path)\nmetadata_df = pd.read_csv(metadata_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Ground Truth DataFrame\"); display(ground_truth_df)\nprint(\"\")\nprint(\"Metadata DataFrame\"); display(metadata_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring the Ground Truth DataFrame.\nprint(f\"Shape of Dataset: {ground_truth_df.shape}\")\nprint(f\"Number of Unique Image Identifiers (ID): {ground_truth_df.image.nunique()}\", end=\"\\n\\n\")\nprint(\"<=======Info=======>\")\nground_truth_df.info(); print()\n\n## Checking the validity of all unique values in the \"image\" column.\nfor idx, image_name in enumerate(ground_truth_df[\"image\"]):\n    if \"ISIC_\" not in image_name: print(f\"Row {idx} has an invalid image name.\")\n\n## Looking at both the unique values for each column and their counts.\nprint(\"<=======Value Counts=======>\")\nfor column in ground_truth_df.columns:\n    display(ground_truth_df[column].value_counts().sort_index()); print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring the Metadata DataFrame.\nprint(f\"Shape of Dataset: {metadata_df.shape}\")\nprint(f\"Number of Unique Image Identifiers (ID): {metadata_df.image.nunique()}\", end=\"\\n\\n\")\nprint(\"<=======Info=======>\")\nmetadata_df.info(); print()\n\n## Checking the validity of all unique values in the \"image\" column.\nfor idx, image_name in enumerate(metadata_df[\"image\"]):\n    if \"ISIC_\" not in image_name: print(f\"Row {idx} has an invalid image name.\")\n\n## Looking at both the unique values for each column and their counts.\nprint(\"<=======Value Counts=======>\")\nfor column in metadata_df.columns:\n    display(metadata_df[column].value_counts().sort_index()); print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring the images folder.\nfiles = os.listdir(train_val_test_img_path)\nprint(f\"Number of files in train folder: {len(files)}\")\nfor file in files:\n    if \".jpg\" not in file:\n        print(f\"Non-Image File Found: {file}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters and Pre-defined Terms","metadata":{}},{"cell_type":"code","source":"classes = [\n    'Melanoma',\n    'Melanocytic nevus',\n    'Basal cell carcinoma',\n    'Actinic keratosis',\n    'Benign keratosis', # Also: (solar lentigo / seborrheic keratosis / lichen planus-like keratosis).\n    'Dermatofibroma',\n    'Vascular lesion',\n    'Squamous cell carcinoma',\n    'Unknown' # Used for unlabelled scans.\n]\n\nclasses_abbrev = [\"MEL\",\"NV\",\"BCC\",\"AK\",\"BKL\",\"DF\",\"VASC\",\"SCC\",\"UNK\"]\n\n# Final classes dictionary which excludes \"Unknown\" classes.\nCLASSES_DICT = dict(tuple(zip(classes_abbrev[:-1], classes[:-1])))\n\nseed = 42\nn_splits = 1\nbatch_size = strategy.num_replicas_in_sync * 20\n\nencoder_name = \"timm-efficientnet-b5\"\nin_channels = 3\ndepth = 5\npretrained_weights = \"noisy-student\"\nin_features = 1024\nstrategy = auto_select_accelerator()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninit_lr = 0.0001\n\nepochs = 20\n\nMODEL_SAVE_PATH = '{}.pth'.format(encoder_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Generator","metadata":{}},{"cell_type":"code","source":"class skin_cancer_ds(Dataset):\n    def __init__(self, df, image_size, mode):\n        super(skin_cancer_ds, self).__init__()\n        self.df = df\n        self.image_size = image_size\n        assert mode in ['train', 'valid', 'test']\n        self.mode = mode\n\n        if self.mode == 'train':\n            self.transform = A.Compose([\n                A.RandomResizedCrop(height=self.image_size, width=self.image_size, scale=(0.25, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1, p=1.0),\n                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=30, interpolation=1, border_mode=0, value=0, p=0.25),\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.OneOf([\n                    A.MotionBlur(p=.2),\n                    A.MedianBlur(blur_limit=3, p=0.1),\n                    A.Blur(blur_limit=3, p=0.1),\n                ], p=0.25),\n                A.Cutout(num_holes=4, max_h_size=32, max_w_size=32, fill_value=0, p=0.25),\n                A.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n                ToTensorV2(),\n            ])\n\n        else:\n            self.transform = A.Compose([\n                A.Resize(self.image_size, self.image_size),\n                A.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n                ToTensorV2(),\n            ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        img_path = train_val_test_img_path + f'/{self.df.loc[index][\"image\"]}.jpg'\n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        image = np.stack([image, image, image], axis=-1)\n        image = self.transform(image=image)[\"image\"]\n        if self.mode in ['train', 'valid']:\n            label = torch.tensor(np.argmax(self.df.loc[index, CLASSES_DICT].values))\n            # label = torch.Tensor(self.df.loc[index, CLASSES_DICT])\n            return image, label\n        else:\n            return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed)\n\n# Splitting train and val from test via a stratified shuffle split.\ntrainval_test_split = StratifiedShuffleSplit(n_splits=n_splits, train_size=0.9, random_state=seed)\nfor train_val_index, test_index in trainval_test_split.split(ground_truth_df[\"image\"].values, \n                                                             np.argmax(ground_truth_df[CLASSES_DICT].values, axis=1)\n):\n    train_val_df = ground_truth_df.loc[train_val_index].reset_index(drop=True)\n    test_df = ground_truth_df.loc[test_index].reset_index(drop=True)\n\n# Splitting train and val via a stratified shuffle split.\ntrain_val_split = StratifiedShuffleSplit(n_splits=n_splits, train_size=0.9, random_state=seed)\nfor train_index, val_index in train_val_split.split(train_val_df[\"image\"].values, \n                                                    np.argmax(train_val_df[CLASSES_DICT].values, axis=1)\n):\n    train_df = train_val_df.loc[train_index].reset_index(drop=True)\n    val_df = train_val_df.loc[val_index].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = skin_cancer_ds(train_df, 384, \"train\")\nval_ds = skin_cancer_ds(val_df, 384, \"valid\")\ntest_ds = skin_cancer_ds(test_df, 384, \"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=RandomSampler(train_ds))\nval_loader = DataLoader(val_ds, batch_size=batch_size, sampler=SequentialSampler(val_ds))\ntest_loader = DataLoader(test_ds, batch_size=batch_size, sampler=SequentialSampler(test_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DS_GCS_PATH = KaggleDatasets().get_gcs_path(\"isic-2019\")  # Trying TPU with tf.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_path = \"/ISIC_2019_Training_Input/ISIC_2019_Training_Input/\"\n\ntrain_paths = [DS_GCS_PATH + tmp_path + image_name + \".jpg\" for image_name in train_df.image.values]\nval_paths = [DS_GCS_PATH + tmp_path + image_name + \".jpg\" for image_name in val_df.image.values]\ntest_paths = [DS_GCS_PATH + tmp_path + image_name + \".jpg\" for image_name in test_df.image.values]\n\ntrain_labels = train_df[CLASSES_DICT].values\nval_labels = val_df[CLASSES_DICT].values\ntest_labels = test_df[CLASSES_DICT].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_decoder(with_labels=True, target_size=(256, 256), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if with_labels else decode\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.9, 1.1)\n        img = tf.image.random_contrast(img, 0.9, 1.1)\n        img = tf.image.random_brightness(img, 0.1)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, bsize=128, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, shuffle=1024, \n                  cache_dir=\"\"):\n    \n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    \n    slices = paths if labels is None else (paths, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n        \n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n    \n    return dset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = build_decoder(with_labels=True, target_size=(512, 512), ext='jpg')\n\ntrain_dataset = build_dataset(\n    train_paths, train_labels, bsize=batch_size, decode_fn=decoder\n)\n\nvalid_dataset = build_dataset(\n    val_paths, val_labels, bsize=batch_size, decode_fn=decoder,\n    shuffle=False, augment=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = keras.Sequential([\n                keras.applications.efficientnet.EfficientNetB3(include_top=False,\n                                                               input_shape=(512, 512, 3)),\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(CLASSES_DICT.keys()), activation='softmax')\n            ])\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss='categorical_crossentropy',\n                  metrics=[tf.keras.metrics.AUC(multi_label=True)])\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n        f'model.h5', save_best_only=True, monitor='loss', mode='min')\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"loss\", patience=3, min_lr=1e-6, mode='min')\n    \nhistory = model.fit(\n    train_dataset, \n    epochs=epochs,\n    verbose=1,\n    callbacks=[checkpoint, lr_reducer])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the Model","metadata":{}},{"cell_type":"code","source":"efnb5_noisy_student_encoder = get_encoder(encoder_name, \n                                          in_channels=in_channels,\n                                          depth=depth,\n                                          weights=pretrained_weights)\n\nclass EfficientNetB5ClsHead(nn.Module):\n    def __init__(self, encoder, in_features):\n        super(EfficientNetB5ClsHead, self).__init__()\n        self.encoder = encoder\n        self.flatten_block = nn.Sequential(*list(self.encoder.children())[-4:])\n        \n        # Note: There seems to be a problem when I just slice the list of children layers. \n        # Deletion works however.\n        del self.encoder.global_pool\n        del self.encoder.act2\n        del self.encoder.bn2\n        del self.encoder.conv_head\n        \n        self.fc = nn.Linear(2048, in_features, bias=True)  \n        self.cls_head = nn.Linear(in_features, len(CLASSES_DICT.keys()), bias=True)\n        \n        # Xavier uniform weight initialization.\n        init.initialize_head(self.fc)\n        init.initialize_head(self.cls_head)\n    \n#     @autocast\n    def forward(self, x):\n        x = self.encoder(x)[-1]  # Output shape: (batch_size, 640, 16, 16).\n        x = self.flatten_block(x)  # Output shape: (batch_size, 2048).\n        x = self.fc(x)  # Output shape: (batch_size, 1024).\n        x = F.relu(x)  # Output shape: (batch_size, 1024).\n        x = F.dropout(x, p=0.5, training=self.training)  # Output shape: (batch_size, 1024).\n        x = self.cls_head(x)  # Output shape: (batch_size, 8).\n        return x\n    \nmodel = EfficientNetB5ClsHead(efnb5_noisy_student_encoder, in_features=in_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model.to(device)\n\n# To handle class imbalance we can weigh each class. \n# Do something like this and pass it into the Loss function:\n\n# CE_weights = torch.zeros(len(CLASSES_DICT.keys()))  # This takes into account the imbalanced dataset.\n# Increment CE_weights e.g. class 0 has 2439 counts then CE_weights[0] has 2439.\n# CE_weights = 1. / CEweights.clamp_(min=1.)  # Weights should be inversely related to count.\n# CE_weights = (CE_weights * numClass / CE_weights.sum()).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=None)\noptimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs-1)\nscaler = torch.cuda.amp.GradScaler()\n\nval_loss_min = np.Inf  # Save model with best performance on val_loss.\n\nrun = wandb.init(project=\"cancer_classification_IH2021\", name=f\"efnb5-noisy-stdnt\")  # Initialize a project.\n\nfor epoch in range(1, epochs+1):\n    scheduler.step()\n    model.train()\n    train_loss = []\n\n    loop = tqdm(train_loader)\n    for images, labels in loop:\n        images = images.to(device)\n        labels = labels.to(device)\n                \n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast(): \n            outputs = model(images)\n            loss = criterion(outputs.float(), labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss.append(loss.item())\n        loop.set_description('Epoch {:02d}/{:02d}'.format(epoch, epochs))\n        loop.set_postfix(loss=np.mean(train_loss))\n        \n        del images, labels\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    train_loss = np.mean(train_loss)\n\n    model.eval()\n\n    val_loss = 0.0\n    for images, labels in tqdm(valid_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        with torch.cuda.amp.autocast(), torch.no_grad():\n            outputs = model(images)\n            loss = criterion(outputs.float(), labels)\n                \n        val_loss += loss.item() * images.size(0)\n            \n        del images, labels\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    val_loss = val_loss / len(valid_loader.dataset)\n            \n    print('train loss: {:.5f} | val_loss: {:.5f}'.format(train_loss, val_loss))\n            \n    wandb.log({\"epoch\": epoch, \n            \"loss\": train_loss, \n            \"val_loss\": val_loss,\n        })\n            \n    if val_loss < val_loss_min:\n        print('Valid loss improved from {:.5f} to {:.5f}, saving model to wandb.'.format(val_loss_min, val_loss))\n        val_loss_min = val_loss\n        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n        artifact = wandb.Artifact(encoder_name, type='model')\n        artifact.add_file(MODEL_SAVE_PATH, name=f\"model{epoch}.pt\")\n        run.log_artifact(artifact)\n        \n    del train_loss\n    gc.collect()\n    torch.cuda.empty_cache()\n        \nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}